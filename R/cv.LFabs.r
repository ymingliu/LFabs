#' A Forward and Backward Stagewise algorithm for High-dimensional smoothed partial rank estimation with sparse laplacian shrinkage.
#'
#' The laplacian shrinkage level is choosed by the cross-validation. This function uses the testing loss to select the turning.
#' @param X The design matrix.
#' @param y The response.
#' @param s The status.
#' @param sigma The smoothing parameter in SPR.
#' @param weight The weight vector of adaptive lasso.
#' @param model The loss function. Quantitative for model="spr", model="cox", model="lm".
#' @param ntau The length of the grid of the turning parameter before the laplacian penalty. Default is 5.
#' @param nfold The \code{nfold}-fold cross-validation. Default is 5.
#'
#' @return A list.
#' \itemize{
#'   \item Beta - The standardized estimator along the solution path.
#'   \item beta - The optimal standardized estimator.
#'   \item lambda - Lambda sequence generated by LFabs.
#'   \item tau - The optimal laplacian shrinkage level choosed by cross-validation.
#'   \item direction - Direction of LFabs. \code{1} indicates a forward step. \code{0} indicates a backward step.
#'   \item active - Active set for each step.
#'   \item iter - Iterations.
#'   \item bic - The bic for each solution.
#'   \item opt - Position of the optimal lambda based on bic.
#' }
#' @seealso \code{LFabs}.
#' @export
#'
#' @examples
#' library(mvtnorm)
#' library(Matrix)
#'
#' n = 400
#' p = 500
#' d = 15
#' g = 5
#' sig = c(0.5, 1.5)
#' rho = 0.9
#' error = "contaminate"
#' tran = "log"
#' censor.rate = 0.1
#' block = "Auto"
#'
#' set.seed(2021)
#' dat = generator(n, p, d, g, sig, rho, error, tran, censor.rate, block)
#' x = dat$x
#' y = dat$y
#' status = dat$status
#'
#' sigma = 1/sqrt(n)
#' w = abs(1/drop(cor(x, y, method = "pearson")))
#' w[which(w=="NaN"|w=="Inf")] = max(w[which(w!="NaN"&w!="Inf")])
#' model = "spr"
#' fit <- cv.LFabs(x, y, status, sigma, w, model)

cv.LFabs <- function(X, y, s = NULL, sigma = NULL, weight = NULL, model = c("spr", "cox", "lm"),
                     ntau = 5, nfold = 5)
{
  x = as.matrix(X)
  model = match.arg(model)

  if (is.null(sigma))           sigma = 1/sqrt(nrow(x))
  if (is.null(weight))         weight = rep(1, ncol(x))
  if (is.null(s)) {
    s = rep(1, nrow(x))
  }

  # generate tau.grid
  fit <- LFabs(x, y, s, sigma, w, model, 0)
  lamb.max = fit$lambda[1]
  tau.max = 5*lamb.max
  tau.min = lamb.max
  tau.grid = exp(seq(log(tau.min), log(tau.max), length.out = ntau))

  # nfold cross-validation
  score = matrix(0, ntau, nfold)
  index = ceiling(sample(1:nrow(x))/nrow(x) * nfold)
  for (ltau in 1:ntau) {
    tau = tau.grid[ltau]
    for (lfold in 1:nfold) {
      tr.id = which(index!=lfold)
      te.id = which(index==lfold)

      x.tr  = as.matrix(x[tr.id,])
      y.tr  = as.vector(y[tr.id])
      s.tr  = as.vector(s[tr.id])
      sigma.tr = 1/sqrt(dim(x.tr)[1])
      w.tr = abs(1/drop(cor(x.tr, y.tr, method = "pearson")))
      w.tr[which(w.tr=="NaN"|w.tr=="Inf")] = max(w.tr[which(w.tr!="NaN"&w.tr!="Inf")])

      x.te  = as.matrix(x[te.id,])
      y.te  = as.vector(y[te.id])
      s.te  = as.vector(s[te.id])
      sigma.te = 1/sqrt(dim(x.te)[1])

      # reorder the testing data
      y.order = order(y.te, decreasing = T)
      x.te  = as.matrix(x.te[y.order,])
      y.te  = as.vector(y.te[y.order])
      s.te  = as.vector(s.te[y.order])

      # train model
      cv.fit <- LFabs(x.tr, y.tr, s.tr, sigma.tr, w.tr, model, tau)

      # test score
      bet = as(cv.fit$beta, "matrix")
      if (model == "spr") {
        for (j in 1:(nrow(x.te)-1)) {
          for (k in (j+1):nrow(x.te)) {
            if (s.te[k]!=0) {
              c = x.te[j,]%*%bet
              d = x.te[k,]%*%bet
              u = (as.numeric(c)-as.numeric(d))/sigma.te
              score[ltau,lfold] = score[ltau,lfold] + 1.0 / (1.0 + exp(-1.0*u))
            }
          }
        }
        score[ltau,lfold] = score[ltau,lfold]/(-nrow(x.te)*(nrow(x.te)-1))
      }
      if (model == "cox") {
        x.tebet = as.vector(x.te%*%bet)
        for (j in 1:nrow(x.te)) {
          if (s.te[j]!=0) {
            d = 0
            for (k in 1:j) {
              d = d + exp(x.tebet[k])
            }
            score[ltau,lfold] = score[ltau,lfold] + 1.0*(x.tebet[j] - log(d))
          }
          score[ltau,lfold] = -score[ltau,lfold]/nrow(x.te)
        }
      }
      if (model == "lm") {
        loss = y.te - x.te %*% bet
        score[ltau,lfold] = colSums(loss^2)/length(y.te)
      }
    }
  }
  opt = which.min(rowMeans(score))

  # fit all data using the optimal tau
  fit = LFabs(x, y, s, sigma, weight, model, tau.grid[opt])

  val = list(Beta        = fit$Beta,
             beta        = fit$beta,
             lambda      = fit$lambda,
             tau         = tau.grid[opt],
             direction   = fit$direction,
             active      = fit$active,
             iter        = fit$iter,
             bic         = fit$bic,
             opt         = fit$opt)

  return(val)
}









